{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Original Notebook from here: lots of mods/fixes since it was 2 years old....\n",
        "https://github.com/spiyer99/spiyer99.github.io/blob/master/nbs/Neural%20Transfer%20of%20Audio%20in%20Pytorch.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qmyqpoXclzT-"
      },
      "source": [
        "Neural Transfer of Audio in Pytorch\n",
        "=========================\n",
        "\n",
        "Neural Style transfer is really interesting. They've been some really interesting applications of style transfer. It basically aims to take the 'style' from one image and change the 'content' image to meet that style. Here's an example. This image has been converted to look like it was painted by Van gough.\n",
        "\n",
        "![photo](https://camo.githubusercontent.com/974884c2fb949b365c3f415b3712d2cac04a35f7/68747470733a2f2f692e696d6775722e636f6d2f575771364931552e6a7067)\n",
        "\n",
        "\n",
        "But so far it hasn't really been applied to audio. So this week I explored the idea of applying neural style transfer to audio. To be frank, the results were less than stellar but I'm hoping to keep working on this in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q7ngoGFMgsIA"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dZzywPCGCXa-"
      },
      "outputs": [],
      "source": [
        "#install issue with torch fixed by: conda install -c defaults intel-openmp -f, something about dll 1% error, https://github.com/pytorch/pytorch/issues/7579\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import youtube_dl \n",
        "import copy\n",
        "import soundfile as sf\n",
        "import time\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "qMAe-s7ohs3A",
        "outputId": "df69f1c4-fdf2-418a-8b38-d5144a3dd755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube-dl in c:\\users\\andrewt02\\.conda\\envs\\pytorch_39\\lib\\site-packages (2021.12.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CGqsFBOVzWwy"
      },
      "source": [
        "# Build dataset (Not used, try with Samples, see below)\n",
        "\n",
        "For this exercise, I'm going to be using clips from the joe rogan podcast. I'm trying to make [joe rogan](https://en.wikipedia.org/wiki/Joe_Rogan), from the [joe rogan podcast](http://podcasts.joerogan.net/), sound like [joey diaz](https://en.wikipedia.org/wiki/Joey_Diaz), from the [Church of Whats Happening Now](https://www.youtube.com/channel/UCv695o3i-JmkUB7tPbtwXDA). Joe Rogan already does a pretty good [impression of joey diaz](https://www.youtube.com/watch?v=SLolljsbbFs). But I'd like to improve his impression using deep learning.\n",
        "\n",
        "First I'm going to download the youtube videos. There's a neat trick mentioned on github that allows you to download small segments of youtube videos. That's handy cause I don't want to download the entire video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "l_HhfsWRviiz"
      },
      "outputs": [],
      "source": [
        "# download youtube url using ffmpeg\n",
        "# adapted from: https://github.com/ytdl-org/youtube-dl/issues/622#issuecomment-162337869\n",
        "# def my_hook(d):\n",
        "#     if d['status'] == 'finished':\n",
        "#         print('Done downloading, now converting ...')\n",
        "\n",
        "# def download_from_url_ffmpeg(url):\n",
        "\n",
        "#     try:\n",
        "#         os.remove(output)\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "#     # cmd = 'ffmpeg -loglevel warning -ss 0 -i $(youtube-dl -f 22 --get-url https://www.youtube.com/watch?v=mMZriSvaVP8) -t 11200 -c:v copy -c:a copy react-spot.mp4'\n",
        "#     # cmd = 'ffmpeg -loglevel warning -ss 0 -i $(youtube-dl -f bestaudio -g '+str(url)+') -t '+str(minute_mark*60)+' '+str(output)\n",
        "#     # os.system(cmd)\n",
        "\n",
        "#     ydl_opts = {\n",
        "#         'format': 'bestaudio/best',\n",
        "#         'postprocessors': [{\n",
        "#             'key': 'FFmpegExtractAudio',\n",
        "#             'preferredcodec': 'wav',\n",
        "#             'preferredquality': '192',\n",
        "#         }],\n",
        "#         # 'logger': MyLogger(),\n",
        "#         'progress_hooks': [my_hook],\n",
        "#         'restrictfilenames':True,\n",
        "#         'forcefilename':True,\n",
        "#         'download_archive': os.path.join(os.getcwd(), \"..\", 'data', 'raw', 'video_downloads', 'archive.txt'),\n",
        "#         'outtmpl': os.path.join(os.getcwd(), \"..\", 'data', 'raw', 'video_downloads', '%(title)s.%(ext)s')\n",
        "#     }\n",
        "#     with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "#         info = ydl.extract_info(url, download=True)\n",
        "#         filename = ydl.prepare_filename(info)\n",
        "\n",
        "#         # correct file name to end in .wav instead of standard m4a\n",
        "#         ext = os.path.splitext(filename)[1]\n",
        "#         if ext != ydl_opts['postprocessors'][0]['preferredcodec']:\n",
        "#             print('wrong codec on file name, resolving.....')\n",
        "#             filename = os.path.splitext(filename)[0] + \".\" + ydl_opts['postprocessors'][0]['preferredcodec']\n",
        "\n",
        "#     return filename\n",
        "\n",
        "\n",
        "# # url = 'https://www.youtube.com/watch?v=-xY_D8SMNtE'\n",
        "# #replacement one without adult restrictions\n",
        "# url = 'https://www.youtube.com/watch?v=HkfuAkM6b_E'\n",
        "# content_audio_name = download_from_url_ffmpeg(url)\n",
        "# url = 'https://www.youtube.com/watch?v=-l88fMJcvWE'\n",
        "# style_audio_name = download_from_url_ffmpeg(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "colab_type": "code",
        "id": "1LL-73kIvjXp",
        "outputId": "dfbb63ec-2b82-4fea-e986-6ccc16e727b2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'content_audio_name' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\notebooks\\2.0-ta-neural-style-transfer-pytorch-proto.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andrewt02/Documents/Python_Scripts/synthegan/notebooks/2.0-ta-neural-style-transfer-pytorch-proto.ipynb#ch0000006?line=0'>1</a>\u001b[0m Audio(content_audio_name)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'content_audio_name' is not defined"
          ]
        }
      ],
      "source": [
        "Audio(content_audio_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "colab_type": "code",
        "id": "DN4ZNYJivlow",
        "outputId": "ef0bab7a-4b6c-4c18-fa84-8c14838facf3"
      },
      "outputs": [],
      "source": [
        "Audio(style_audio_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# spoke joe rogan examples\n",
        "# content_audio_name = r'C:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\data\\raw\\video_downloads\\Joe_Rogan_Meets_Roe_Jogan_I.wav'\n",
        "# style_audio_name = r'C:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\data\\raw\\video_downloads\\Joey_Diaz_s_Food_Talk_and_Roasts_Lee_Again_About_Eating_Choices.wav'\n",
        "\n",
        "#hip hop sample\n",
        "content_audio_name = r'C:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\data\\raw\\hip_hop_essentials\\beats\\Bonus Beat 18 (95BPM).wav'\n",
        "# 80's style\n",
        "style_audio_name = r'C:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\data\\raw\\80s_synth_samples\\bass_loops\\105bpm\\80s_SynclavBassB[105]-G.wav'\n",
        "\n",
        "audio_duration = 10 #s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JzJ1sybvNj-7"
      },
      "source": [
        "# Loss\n",
        "\n",
        "There are two types of loss for this:\n",
        "\n",
        "1. Content loss. Lower values for this means that the output audio sounds like joe rogan. \n",
        "\n",
        "2. Style loss. Lower values for this means that the output audio sounds like joey diaz.\n",
        "\n",
        "Ideally we want both content and style loss to be minimised.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "laOvOlsaRgmc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using: \" + str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c3y373ZVgs1X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\andrewt02\\.conda\\envs\\pytorch_39\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\andrewt02\\.conda\\envs\\pytorch_39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Conv2d, ReLU, AvgPool1d, MaxPool2d, Linear, Conv1d\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "import librosa\n",
        "\n",
        "\n",
        "\n",
        "class GramMatrix(nn.Module):\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\ta, b, c = input.size()  # a=batch size(=1)\n",
        "\t\t\t\t# b=number of feature maps\n",
        "\t\t\t\t# (c,d)=dimensions of a f. map (N=c*d)\n",
        "\t\tfeatures = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
        "\t\tG = torch.mm(features, features.t())  # compute the gram product\n",
        "\t\t\t\t# we 'normalize' the values of the gram matrix\n",
        "\t\t\t\t# by dividing by the number of element in each feature maps.\n",
        "\t\treturn G.div(a * b * c)\n",
        "\t\n",
        "\n",
        "# https://ghamrouni.github.io/stn-tuto/advanced/neural_style_tutorial.html#\n",
        "class ContentLoss(nn.Module):\n",
        "\n",
        "\t\tdef __init__(self, target, weight):\n",
        "\t\t\t\tsuper(ContentLoss, self).__init__()\n",
        "\t\t\t\t# we 'detach' the target content from the tree used\n",
        "\t\t\t\tself.target = target.detach() * weight\n",
        "\t\t\t\t# to dynamically compute the gradient: this is a stated value,\n",
        "\t\t\t\t# not a variable. Otherwise the forward method of the criterion\n",
        "\t\t\t\t# will throw an error.\n",
        "\t\t\t\tself.weight = weight\n",
        "\t\t\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\t\tdef forward(self, input):\n",
        "\t\t\t\tself.loss = self.criterion(input * self.weight, self.target)\n",
        "\t\t\t\tself.output = input\n",
        "\t\t\t\treturn self.output\n",
        "\n",
        "\t\tdef backward(self, retain_graph=True):\n",
        "\t\t\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\t\t\treturn self.loss\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "\tdef __init__(self, target, weight):\n",
        "\t\tsuper(StyleLoss, self).__init__()\n",
        "\t\tself.target = target.detach() * weight\n",
        "\t\tself.weight = weight\n",
        "\t\tself.gram = GramMatrix()\n",
        "\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tself.output = input.clone()\n",
        "\t\tself.G = self.gram(input)\n",
        "\t\tself.G.mul_(self.weight)\n",
        "\t\tself.loss = self.criterion(self.G, self.target)\n",
        "\t\treturn self.output\n",
        "\n",
        "\tdef backward(self,retain_graph=True):\n",
        "\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\treturn self.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ABlfWrmJcNQF"
      },
      "source": [
        "# Converting Wav to Matrix\n",
        "\n",
        "To convert the waveform audio to a matrix that we can pass to pytorch I'll use `librosa`. Most of this code was borrowed from Dmitry Ulyanov's [github repo](https://github.com/DmitryUlyanov/neural-style-audio-tf/blob/master/neural-style-audio-tf.ipynb) and Alish Dipani's [github repo](https://github.com/alishdipani/Neural-Style-Transfer-Audio). \n",
        "\n",
        "We get the Short-time Fourier transform from the audio using the `librosa` library. The window size for this is `2048`, which is also the default setting. There is scope here for replacing the code with code from torchaudio. But this works for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "8FtFef7v7iER",
        "outputId": "828b9dd0-87e7-484f-cc11-5f7cae93f066"
      },
      "outputs": [],
      "source": [
        "import gc; gc.collect()\n",
        "\n",
        "N_FFT = 256 # originally 2048, 512 reommended for voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w0Yh70aGP6-6"
      },
      "outputs": [],
      "source": [
        "# USING LIBROSA\n",
        "def read_audio_spectum(filename):\n",
        "  x, fs = librosa.load(filename,duration=audio_duration)\n",
        "  S = librosa.stft(x, N_FFT)\n",
        "  p = np.angle(S)\n",
        "  S = np.log1p(np.abs(S))  \n",
        "  return S, fs\n",
        "\n",
        "style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
        "content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
        "\n",
        "if(content_sr != style_sr):\n",
        "  raise 'Sampling rates are not same'\n",
        "\n",
        "  \n",
        "style_audio = style_audio.reshape([1,int(N_FFT/2 + 1),style_audio.shape[1]])\n",
        "content_audio = content_audio.reshape([1,int(N_FFT/2 + 1),content_audio.shape[1]])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "  content_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
        "else:\n",
        "  style_float = Variable(torch.from_numpy(style_audio))\n",
        "  content_float = Variable(torch.from_numpy(content_audio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mc0_sxe0Ustu"
      },
      "outputs": [],
      "source": [
        "# !pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rFGBnAZUqr_W"
      },
      "source": [
        "# Create CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uCu9bCbE1Kad"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cnn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\notebooks\\2.0-ta-neural-style-transfer-pytorch-proto.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andrewt02/Documents/Python_Scripts/synthegan/notebooks/2.0-ta-neural-style-transfer-pytorch-proto.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgc\u001b[39;00m; gc\u001b[39m.\u001b[39mcollect(); \u001b[39mdel\u001b[39;00m cnn\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cnn' is not defined"
          ]
        }
      ],
      "source": [
        "import gc; gc.collect(); del cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z2iadNf018iE"
      },
      "source": [
        "This CNN is very shallow. It consists of 2 convolutions and a ReLU in between them. I originally took the CNN used [here](https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py) but I've made a few changes. \n",
        "\n",
        " - Firstly, I added content loss. This wasn't added before and is obviously very useful. We'd like to know how close (or far away) the audio sounds to the original content.\n",
        "\n",
        " - Secondly, I added a ReLU to the model. It's pretty well [established](https://stats.stackexchange.com/questions/275358/why-is-increasing-the-non-linearity-of-neural-networks-desired) that nonlinear activations are desired in a neural network. Adding a ReLU improved the model significantly.\n",
        "\n",
        " - Increased the number of steps. From ``2500`` to `20000`\n",
        "\n",
        " - Slightly deepened the network. I added a layer of `Conv1d`. After this layer style loss and content loss is calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xGPubl9D3zrr"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, N_FFT):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # self.cnn1 = Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "        self.cnn1 = Conv1d(in_channels=int(N_FFT/2+1), out_channels=2*N_FFT, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = ReLU()\n",
        "        # self.cnn2 = Conv1d(in_channels=4096, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "        self.cnn2 = Conv1d(in_channels=2*N_FFT, out_channels=2*N_FFT, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.cnn2(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.cnn3(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "colab_type": "code",
        "id": "Q7RqNYVhCQ3H",
        "outputId": "7d3b915a-c191-42f5-de52-702d8250f7de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Sequential(\n",
              "   (conv_1): Conv1d(129, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "   (relu1): ReLU()\n",
              "   (conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "   (style_loss_1): StyleLoss(\n",
              "     (gram): GramMatrix()\n",
              "     (criterion): MSELoss()\n",
              "   )\n",
              "   (content_loss_1): ContentLoss(\n",
              "     (criterion): MSELoss()\n",
              "   )\n",
              " ),\n",
              " [StyleLoss(\n",
              "    (gram): GramMatrix()\n",
              "    (criterion): MSELoss()\n",
              "  )],\n",
              " [ContentLoss(\n",
              "    (criterion): MSELoss()\n",
              "  )])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn = CNNModel(N_FFT)\n",
        "if torch.cuda.is_available():\n",
        "  cnn = cnn.cuda()\n",
        "\n",
        "\n",
        "style_weight=100\n",
        "content_weight = 2\n",
        "\n",
        "\n",
        "def get_style_model_and_losses(cnn, style_float,\\\n",
        "                               content_float=content_float,\\\n",
        "                               style_weight=style_weight):\n",
        "  \n",
        "  cnn = copy.deepcopy(cnn)\n",
        "\n",
        "  style_losses = []\n",
        "  content_losses = []\n",
        "\n",
        "  # create model\n",
        "  model = nn.Sequential()\n",
        "\n",
        "  # we need a gram module in order to compute style targets\n",
        "  gram = GramMatrix()\n",
        "\n",
        "  # load onto gpu  \n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    gram = gram.cuda()\n",
        "\n",
        "  # add conv1\n",
        "  name = 'conv_1'\n",
        "  model.add_module(name, cnn.cnn1)\n",
        "\n",
        "  # add relu\n",
        "  name = 'relu1'\n",
        "  model.add_module(name, cnn.relu)\n",
        "\n",
        "  # add conv2\n",
        "  name = 'conv_2'\n",
        "  model.add_module(name, cnn.cnn2)\n",
        "\n",
        "  # add style loss\n",
        "  target_feature = model(style_float).clone()\n",
        "  target_feature_gram = gram(target_feature)\n",
        "  style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "  model.add_module(\"style_loss_1\", style_loss)\n",
        "  style_losses.append(style_loss)\n",
        "\n",
        "  # add content loss\n",
        "  target = model(content_float).detach()\n",
        "  content_loss = ContentLoss(target, content_weight)\n",
        "  model.add_module(\"content_loss_1\", content_loss)\n",
        "  content_losses.append(content_loss)\n",
        "\n",
        "  return model, style_losses, content_losses\n",
        "\n",
        "\n",
        "get_style_model_and_losses(cnn, style_float, content_float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pGPgaB-1fie8"
      },
      "source": [
        "# Run style transfer\n",
        "\n",
        "Now I'll run the style transfer. This will use the `optim.Adam` optimizer. This piece of code was taken from the pytorch tutorial for [neural style transfer](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html). For each iteration of the network the style loss and content loss is calculated. In turn that is used to get the gradients. The gradients are mulitplied by the learnign rates. That in turn updates the input audio matrix. In pytorch the optimizer requries a [closure](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html#gradient-descent) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_wav_result(output, epoch, style_loss, content_loss):\n",
        "    # taken from: https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        output = output.cpu()\n",
        "\n",
        "    output = output.squeeze(0)\n",
        "    output = output.numpy()\n",
        "\n",
        "    a = np.zeros_like(output)\n",
        "    a = np.exp(output) - 1\n",
        "\n",
        "    # This code is supposed to do phase reconstruction\n",
        "    p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "    for i in range(1000):\n",
        "        S = a * np.exp(1j*p)\n",
        "        x = librosa.istft(S)\n",
        "        p = np.angle(librosa.stft(x, N_FFT))\n",
        "    \n",
        "    OUTPUT_FILENAME = os.path.join(run_result_dir, f'output_{epoch}_SL{style_loss:.5f}_CL{content_loss:.5f}_{time.strftime(\"%Y%m%d-%H%M%S\")}.wav')\n",
        "    sf.write(OUTPUT_FILENAME, x, style_sr)\n",
        "    # Audio(OUTPUT_FILENAME)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "908CdwL0wXjQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building the style transfer model..\n",
            "Optimizing..\n",
            "run [100]:\n",
            "\tStyle Loss : 0.000270 Content Loss: 0.008086\n",
            "\tDuration: 0.699s\n",
            "run [200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008042\n",
            "\tDuration: 0.674s\n",
            "run [300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008038\n",
            "\tDuration: 0.669s\n",
            "run [400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008038\n",
            "\tDuration: 0.676s\n",
            "run [500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008038\n",
            "\tDuration: 0.672s\n",
            "run [600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008038\n",
            "\tDuration: 0.670s\n",
            "run [700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.674s\n",
            "run [800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.672s\n",
            "run [900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.675s\n",
            "run [1000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 37.066s\n",
            "run [1100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 1.532s\n",
            "run [1200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.667s\n",
            "run [1300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.671s\n",
            "run [1400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.676s\n",
            "run [1500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008037\n",
            "\tDuration: 0.672s\n",
            "run [1600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.671s\n",
            "run [1700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.668s\n",
            "run [1800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.674s\n",
            "run [1900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.673s\n",
            "run [2000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 36.779s\n",
            "run [2100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 1.519s\n",
            "run [2200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.665s\n",
            "run [2300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.671s\n",
            "run [2400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.675s\n",
            "run [2500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.667s\n",
            "run [2600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.673s\n",
            "run [2700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.674s\n",
            "run [2800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.671s\n",
            "run [2900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.676s\n",
            "run [3000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 35.538s\n",
            "run [3100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 1.539s\n",
            "run [3200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.667s\n",
            "run [3300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.673s\n",
            "run [3400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008036\n",
            "\tDuration: 0.671s\n",
            "run [3500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.673s\n",
            "run [3600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.672s\n",
            "run [3700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.673s\n",
            "run [3800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.671s\n",
            "run [3900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.672s\n",
            "run [4000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 39.732s\n",
            "run [4100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 1.536s\n",
            "run [4200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [4300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.673s\n",
            "run [4400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [4500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [4600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [4700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.671s\n",
            "run [4800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.673s\n",
            "run [4900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [5000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 40.898s\n",
            "run [5100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 1.534s\n",
            "run [5200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [5300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [5400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.669s\n",
            "run [5500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.667s\n",
            "run [5600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [5700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [5800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.669s\n",
            "run [5900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.672s\n",
            "run [6000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 40.118s\n",
            "run [6100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 1.517s\n",
            "run [6200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.671s\n",
            "run [6300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [6400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [6500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [6600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.669s\n",
            "run [6700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [6800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.668s\n",
            "run [6900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [7000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 37.277s\n",
            "run [7100]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 1.530s\n",
            "run [7200]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.665s\n",
            "run [7300]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.671s\n",
            "run [7400]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [7500]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.666s\n",
            "run [7600]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008035\n",
            "\tDuration: 0.670s\n",
            "run [7700]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008034\n",
            "\tDuration: 0.672s\n",
            "run [7800]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008034\n",
            "\tDuration: 0.666s\n",
            "run [7900]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008034\n",
            "\tDuration: 0.675s\n",
            "run [8000]:\n",
            "\tStyle Loss : 0.000269 Content Loss: 0.008034\n",
            "\tDuration: 37.711s\n"
          ]
        }
      ],
      "source": [
        "import gc; gc.collect()\n",
        "\n",
        "input_float = content_float.clone()\n",
        "#input_float = Variable(torch.randn(content_float.size())).type(torch.FloatTensor)\n",
        "\n",
        "learning_rate_initial = 1e-2\n",
        "\n",
        "def get_input_param_optimizer(input_float):\n",
        "  input_param = nn.Parameter(input_float.data)\n",
        "  # optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
        "  optimizer = optim.Adam([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.LBFGS([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.SGD([input_param], lr=learning_rate_initial)\n",
        "  # optimizer = optim.RMSprop([input_param], lr=learning_rate_initial)\n",
        "  return input_param, optimizer\n",
        "\n",
        "num_steps= 8000\n",
        "\n",
        "run_result_dir = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\", \"style_transfer_training\", f'run_{time.strftime(\"%Y%m%d-%H%M%S\")}')\n",
        "os.mkdir(run_result_dir)\n",
        "\n",
        "shutil.copy(content_audio_name, run_result_dir)\n",
        "shutil.copy(style_audio_name, run_result_dir)\n",
        "\n",
        "# from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
        "def run_style_transfer(cnn, style_float=style_float,\\\n",
        "                       content_float=content_float,\\\n",
        "                       input_float=input_float,\\\n",
        "                       num_steps=num_steps, style_weight=style_weight): \n",
        "    print('Building the style transfer model..')\n",
        "    # model, style_losses = get_style_model_and_losses(cnn, style_float)\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(cnn, style_float, content_float)\n",
        "    input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "    print('Optimizing..')\n",
        "    run = [0]\n",
        "    start_time = time.time()\n",
        "    while run[0] <= num_steps:\n",
        "\n",
        "        def closure():\n",
        "            # correct the values of updated input image\n",
        "            input_param.data.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_param)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                #print('sl is ',sl,' style loss is ',style_score)\n",
        "                style_score += sl.loss\n",
        "\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            style_score *= style_weight\n",
        "            content_score *= content_weight\n",
        "\n",
        "            loss = style_score + content_score\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            if run[0] % 100 == 0:\n",
        "                print(f\"run {run}:\")\n",
        "                print('\\tStyle Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                            style_score.item(), content_score.item()))\n",
        "\n",
        "            if run[0] % 1000 == 0:\n",
        "                save_wav_result(input_param.data, run[0], style_score.item(), content_score.item())\n",
        "\n",
        "            return style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "        if run[0] % 100 == 0:\n",
        "            print(f'\\tDuration: {time.time() - start_time:.3f}s')\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "            \n",
        "    # ensure values are between 0 and 1\n",
        "    input_param.data.clamp_(0, 1)\n",
        "\n",
        "    return input_param.data\n",
        "\n",
        "\n",
        "output = run_style_transfer(cnn, style_float=style_float, content_float=content_float, input_float=input_float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SmlYYpFSq9Tr"
      },
      "source": [
        "# Reconstruct Audio\n",
        "\n",
        "Finally the audio needs to be reconstructed. To do that the librosa inverse short-time fourier transform can be used. \n",
        "\n",
        "Then we write to an audio file and use the jupyter notebook extension to play the audio in the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "F5rtCog9cjqC"
      },
      "outputs": [],
      "source": [
        "# taken from: https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  output = output.cpu()\n",
        "\n",
        "output = output.squeeze(0)\n",
        "output = output.numpy()\n",
        "\n",
        "N_FFT=2048\n",
        "a = np.zeros_like(output)\n",
        "a = np.exp(output) - 1\n",
        "\n",
        "# This code is supposed to do phase reconstruction\n",
        "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "for i in range(500):\n",
        "  S = a * np.exp(1j*p)\n",
        "  x = librosa.istft(S)\n",
        "  p = np.angle(librosa.stft(x, N_FFT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LlFbGxFUpcFq"
      },
      "outputs": [],
      "source": [
        "OUTPUT_FILENAME = 'output.wav'\n",
        "sf.write(OUTPUT_FILENAME, x, style_sr)\n",
        "# Audio(OUTPUT_FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Sf6Q4fXYgyIb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\andrewt02\\Documents\\Python_Scripts\\synthegan\\notebooks\\2.0-ta-neural-style-transfer-pytorch-proto.ipynb Cell 30'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andrewt02/Documents/Python_Scripts/synthegan/notebooks/2.0-ta-neural-style-transfer-pytorch-proto.ipynb#ch0000029?line=5'>6</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andrewt02/Documents/Python_Scripts/synthegan/notebooks/2.0-ta-neural-style-transfer-pytorch-proto.ipynb#ch0000029?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andrewt02/Documents/Python_Scripts/synthegan/notebooks/2.0-ta-neural-style-transfer-pytorch-proto.ipynb#ch0000029?line=7'>8</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "cnn = None\n",
        "output = None\n",
        "style_float = None\n",
        "content_float = None\n",
        "input_float = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "alishdipani_audio_torch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
